{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial investigation of some survival analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from phmlondon.snow_utils import SnowflakeConnection\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data\n",
    "First we have to make a snowflake connection and join the admissions table onto the feature table/ yearly feature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "snowsesh = SnowflakeConnection()\n",
    "snowsesh.use_database(\"INTELLIGENCE_DEV\")\n",
    "snowsesh.use_schema(\"AI_CENTRE_FEATURE_STORE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_query = \"\"\"\n",
    "with pheno_year as\n",
    "\n",
    "-- Pull out the most recent year of observations for our cohort\n",
    "(select distinct \n",
    "person_id,\n",
    "max(observation_year) observation_year\n",
    "from INTELLIGENCE_DEV.AI_CENTRE_FEATURE_STORE.PERSON_PHENOTYPE_BY_YEAR\n",
    "where observation_year between 2019 and 2020\n",
    "group by person_id)\n",
    "\n",
    "select * from \n",
    "INTELLIGENCE_DEV.AI_CENTRE_FEATURE_STORE.PERSON_PHENOTYPE_BY_YEAR pheno\n",
    "inner join pheno_year on pheno_year.person_id = pheno.person_id and pheno_year.observation_year = pheno.observation_year\n",
    "join INTELLIGENCE_DEV.AI_CENTRE_FEATURE_STORE.COHORT_TABLE cohort on pheno.person_id = cohort.person_id\n",
    "where cohort.admission_date between '2021-01-01' and '2022-01-01' or cohort.admission_date is null\n",
    "\n",
    "\"\"\"\n",
    "cohort_table = snowsesh.execute_query_to_df(cohort_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take only one admission per person\n",
    "one_admission = cohort_table[~cohort_table.PERSON_ID.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_admission.ADMISSION_DATE.isna()[~missing_rows].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_columns = ['LONDON_IMD_DECILE',        \n",
    "                     'ASTHMA', \n",
    "                     'COPD', \n",
    "                     'DIABETES_TYPE2', \n",
    "                     'DIABETES_TYPE1', \n",
    "                     'HYPERTENSION',       \n",
    "                     'CORONARY_HEART_DISEASE',\n",
    "                     'STROKE',\n",
    "                     'CKD_STAGE3', \n",
    "                     'SEVERE_MENTAL_ILLNESS',       \n",
    "                     'CANCER', \n",
    "                     'DEMENTIA', \n",
    "                     'ATRIAL_FIBRILLATION', \n",
    "                     'PALLIATIVE_CARE',       \n",
    "                     'HEART_FAILURE', \n",
    "                     'PATIENT_AGE_AT_ACTIVITY'\n",
    "                     ]\n",
    "\n",
    "#Make dummy cols for modelling - commented out as duplicate columns here\n",
    "#one_admission_inputs = pd.get_dummies(one_admission.loc[:, modelling_columns], \n",
    "#                                      columns=['ETHNIC_AIC_CATEGORY', 'GENDER'], \n",
    "#                                      drop_first = True)\n",
    "one_admission_inputs = one_admission.loc[:, modelling_columns]\n",
    "\n",
    "#Drop any columns with missing data\n",
    "missing_rows = one_admission_inputs.isna().any(axis=1)\n",
    "logit_model = sm.Logit(~one_admission.ADMISSION_DATE.isna()[~missing_rows],\n",
    "                       one_admission_inputs[~missing_rows].astype(float))\n",
    "\n",
    "logit_res = logit_model.fit()\n",
    "\n",
    "print(logit_res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
